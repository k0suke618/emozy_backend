{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"HF_DATASETS_DISABLE_MULTIPROCESSING\"] = \"1\"\n",
    "\n",
    "\n",
    "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4ビット量子化済みモデル（高速ダウンロード＆OOM回避）\n",
    "\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "\n",
    "dtype = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fourbit_models = [\n",
    "\n",
    "\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",  # bitsandbytes 4ビット量子化\n",
    "\n",
    "\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "\n",
    "\n",
    "    \"unsloth/gpt-oss-20b\",                    # MXFP4フォーマット\n",
    "\n",
    "\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6ae4e",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b02022",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    dtype = dtype,  # 自動検出\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,  # 4ビット量子化でメモリ削減\n",
    "    full_finetuning = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # 0以上の値を入れる。8, 16, 32, 64, 128 など\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # なんでもいいが 0 が最適らしい\n",
    "    bias = \"none\",    # なんでもいいが \"none\" が最適らしい\n",
    "    # ↓ \"unsloth\" にすると 30% 少ない VRAM かつ倍のサイズのバッチサイズにできるらしい\n",
    "    use_gradient_checkpointing = \"unsloth\", # 長いコンテキスト長の時は True or \"unsloth\" にする\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # rank stabilized LoRA\n",
    "    loftq_config = None, # LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf3906",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88288050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     convos = examples[\"messages\"]\n",
    "#     texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "#              for convo in convos]\n",
    "#     return {\"text\": texts}\n",
    "\n",
    "# # データセット読み込み\n",
    "# dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "# # dataset = load_dataset(\"teaching_data.jsonl\")\n",
    "\n",
    "\n",
    "# # データセットの標準化とフォーマット\n",
    "# dataset = standardize_sharegpt(dataset)\n",
    "# dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# # 最初のサンプルを確認\n",
    "# print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef832f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "# dataset = []\n",
    "\n",
    "# with open(\"teaching_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         data = json.loads(line)\n",
    "#         dataset.append(data)\n",
    "# print(type(dataset[0]))\n",
    "# print(dataset[0]['text'])\n",
    "\n",
    "# class 'datasets.arrow_dataset.Dataset'に変換\n",
    "dataset = load_dataset(\"json\", data_files=\"teaching_data.jsonl\")[\"train\"]\n",
    "\n",
    "# randomにシャッフル\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "for i in range(10):\n",
    "    print(dataset[i]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f753225",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 30,\n",
    "        max_steps = 886,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        dataset_num_proc = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在のメモリ統計を表示\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. 最大メモリ = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB のメモリが予約されています。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d708f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練開始\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# 訓練後のメモリと時間の統計\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "print(f\"訓練時間: {round(trainer_stats.metrics['train_runtime']/60, 2)} 分\")\n",
    "print(f\"ピーク使用メモリ: {used_memory} GB\")\n",
    "print(f\"LoRA訓練用メモリ: {used_memory_for_lora} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80327fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"reasoning language: French\\n\\nあなたは数学の問題を解決できる有用なアシスタントです。\"\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"x^5 + 3x^4 - 10 = 3を解いてください。\"},\n",
    "# ]\n",
    "\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True,\n",
    "#     return_tensors = \"pt\",\n",
    "#     return_dict = True,\n",
    "#     reasoning_effort = \"medium\",\n",
    "# ).to(model.device)\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# _ = model.generate(**inputs, max_new_tokens = 2048, streamer = TextStreamer(tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
